The app we’ll build during the meetup is a lightweight aggregator for Madison Common Council blog posts—a feature the city’s website used to provide but no longer does. On a scheduled interval (likely via a GitHub Action), the app will fetch all 20 alder blogs, extract new posts, and publish them to a static site on GitHub Pages. The site will support filtering by alder, narrowing results by date range, and perhaps some simple text search over recent content. Because everything is client-side, search will likely be limited to the most recent window of posts, but it should be more than enough for casual browsing. We’ll also generate an RSS feed for those who prefer old-school syndication.

Behind the scenes, the project is a classic example of Simon Willison’s “[git-scraping.](https://simonwillison.net/2020/Oct/9/git-scraping/)” Each run stores a lightweight history of all discovered posts - URL, title, and perhaps the first few paragraphs - in version control. This not only simplifies incremental processing but also gives us a durable history of council communications with zero backend infrastructure. As a stretch goal, we may even pipe new posts through an LLM to generate short summaries, or post them to a Bluesky bot account.

We'll have a PRD document in Git ahead of time, but folks might want to start from this shorter version instead and let their tools create their own PRD and/or implementation plan - it's up to each presenter. Each presenter will get their own Github repo to build and deploy in.

To save some time, we have looked over the web page structure for Alder blogs, and you can find more details in [SCRAPING_INFO.md](./SCRAPING_INFO.md). The base URL for a blog site is https://www.cityofmadison.com/council/districtXX/blog

Because the project is small, well-scoped, and fully public-data-driven, it’s an ideal live “AI coding tools bake-off.” Each presenter will demonstrate how their chosen tool-Copilot, Cursor, Claude Code, Gemini, Codex, or others-approaches the same problem from scratch. Attendees will see how different models interpret prompts, scaffold code, implement scraping logic, set up scheduling, and handle the filtering and search features. The app is intentionally simple - something most programmers could imagine themselves building - but watching 3 or 4 AI coding assistants independently build it in twenty-minute bursts should give newcomers a grounded sense of how these tools work in practice and help them choose one with confidence.